---
title: "*Feature Screening*"
author: "F. Chiaromonte (48.5%), L. Insolia (48.5%), L. Testa (1%)"
date: "March 23rd 2023"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
```

\section{Introduction}

\subsection{Libraries}

We are going to use:

```{r}
# auto-install SIS if needed
if(!require(SIS)){install.packages("SIS", dep=T); library(SIS)}

library(glmnet)     # elastic net for GLMs
library(mvtnorm)    # to generate multivariate normal distributions
library(corrplot)   # correlation plot
library(tidyverse)  # data manipulation and visualization
library(caret)      # classification learning (confusion matrix)
```

\subsection{Data}

We will simulate some Gaussian data for an example with continuous response.

We will also analyze Leukemia data (with a binary response) that are provided in the SIS package:
- leukemia.test: Gene expression 7129 genes from 34 patients with acute leukemias (20 in class Acute Lymphoblastic Leukemia and 14 in class Acute Myeloid Leukemia).
- leukemia.train: Gene expression of 7129 genes from 38 patients with acute leukemias (27 in class Acute Lymphoblastic Leukemia and 11 in class Acute Myeloid Leukemia).

\section{Linear regression}

\subsection{``Ideal'' scenario}

Let's generate some data with strong signals and low collinearity:
```{r}

set.seed(1) # Do you remember this command???

n <- 100   # obs
p <- 2000  # predictors
pnot <- 5  # relevant predictors

# signal to noise ratio SNR = var(Xb)/var(err) -- similar to R^2 but unbounded
SNR <- 3  # => R^2 approx 0.8

# regression coefficients
b <- rep(0, p)
b[1:pnot] <- 0.5

# create a random matrix X (mean zero and uncorrelated predictors with unit variance)
mu <- rep(0, p)
sigma <- diag(p)
X <- rmvnorm(n, mu, sigma)

# strongest (spurious) correlations
corx <- cor(X)
corxtri <- corx[upper.tri(cor(X))]
scorx <- sort(corxtri)
head(scorx)
tail(scorx)
plot(1:(p*(p-1)/2), scorx, type="l")
abline(h=0, col="red", lty=2) 

hist(scorx)

# plot correlations in X (just take 50 predictors at random)
randp <- sample(1:p, 50)
corrplot(corx[randp,randp], tl.pos='n', type="upper", diag=F)

# true predictions
truepred <- X %*% b
# variance of the error term according to the SNR
varerr <- var(truepred)/SNR
# generate the error
err <- rnorm(n)*sqrt(varerr)

# create the response
y <- truepred + err

# sanity check
varerr
var(err)
summary(lm(y ~ X[, 1:pnot]))
# summary(lm(y ~ X[, 1:p]))

y.train <- y
x.train <- X

```

Let's check the SIS function.
It first implements the Iterative Sure Independence Screening for different variants of (I)SIS, and then fits the final regression model using the R packages ncvreg and glmnet for the SCAD/MCP/LASSO regularized loglikelihood for the variables picked by (I)SIS.

```{r}
help(SIS)
```

We now perform variable selection using:
- SIS: in its vanilla version
- ISIS: the iterated version of SIS

These are paired with the LASSO penalty and the ten-fold cross-validation method for choosing the regularization parameter.

```{r}

# num of features retained by SIS/ISIS
# q <- n-1
q <- round(n / log(n))
q

# maximum num of iterations for ISIS
maxit <- 10

# vanilla SIS
model1 = SIS(x.train, y.train, 
             penalty = "lasso",
             varISIS = "vanilla", iter=F,
             tune = "cv", nfolds = 10, nsis=q,
             standardize = FALSE, seed = 1)

# vanilla ISIS
model2 = SIS(x.train, y.train,
             penalty = "lasso",
             varISIS = "vanilla", iter=T, iter.max=maxit,
             tune = "cv", nfolds = 10, nsis=q,
             standardize = FALSE, seed = 1)
```

Lets'see the selected predictors:
```{r}

cat("\n\n\n")
print(paste0("SIS selected features: ", length(model1$ix0)))
model1$ix0

cat("\n\n\n")
print(paste0("SIS+Lasso selected features: ", length(model1$ix)))
model1$ix

cat("\n\n\n")
print(paste0("ISIS selected features: ", length(model2$ix0)))
model2$ix0

cat("\n\n\n")
print(paste0("ISIS+Lasso selected features: ", length(model2$ix)))
model2$ix

```

Compare it with a Lasso fit.
Here is the overall path:
```{r}
modelLasso = glmnet(x.train, y.train, standardize = F)
plot(modelLasso, xvar="lambda")
```

Let's select the tuning parameter:
```{r}
set.seed(1)

modelLassoCV = cv.glmnet(x.train, y.train, standardize = F)
plot(modelLassoCV)

```

Extract the associated features
```{r}

modelLasso = glmnet(x.train, y.train, standardize = F, 
                    lambda=modelLassoCV$lambda.1se)
Lassocoef <- which(coef(modelLasso) != 0)
Lassocoef

# compare them with SIS
Lassocoef %in% model1$ix

# compare them with ISIS
Lassocoef %in% model2$ix

```


Let's see if we can retrieve the "truly" relevant features using nonconvex penalization methods (MCP):
```{r}

# vanilla SIS
model1 = SIS(x.train, y.train, 
             penalty = "MCP",
             varISIS = "vanilla", iter=F,
             tune = "cv", nsis=q,
             standardize = FALSE, seed = 1)

# vanilla ISIS
model2 = SIS(x.train, y.train,
             penalty = "MCP",
             varISIS = "vanilla", iter=T, iter.max=maxit,
             tune = "cv", nsis=q,
             standardize = FALSE, seed = 1)
```

Let's see the results:
```{r}

cat("\n\n\n")
print(paste0("SIS selected features: ", length(model1$ix0)))
model1$ix0

cat("\n\n\n")
print(paste0("SIS+MCP selected features: ", length(model1$ix)))
model1$ix

cat("\n\n\n")
print(paste0("ISIS selected features: ", length(model2$ix0)))
model2$ix0

cat("\n\n\n")
print(paste0("ISIS+MCP selected features: ", length(model2$ix)))
model2$ix

```


\subsection{Stronger collinearity}

Let's generate some data as before, but with stronger collinearity.
Namely, $\Sigma_x$ has an autoregressive correlation structure, 
where $\Sigma_{x,ij} = \rho^{|i-j|}$ for $\rho=0.6$:
```{r}

set.seed(1)

n <- 100   # obs
p <- 2000  # predictors
pnot <- 5  # relevant predictors

# signal to noise ratio SNR = var(Xb)/var(err) -- similar to R^2 but unbounded
SNR <- 3  # => R^2 approx 0.8

# regression coefficients
b <- rep(0, p)
b[1:pnot] <- 0.5

# create a random matrix X (mean zero and autoregressive correlation)
rho <- 0.6
mu <- rep(0, p)
sigma <- matrix(NA, p, p)
for (i in 1:p) {
  for (j in 1:p) {
    sigma[i,j] <- rho^abs(i-j)
  }
}  
X <- rmvnorm(n, mu, sigma)

# strongest correlations
corx <- cor(X)
corxtri <- corx[upper.tri(cor(X))]
scorx <- sort(corxtri)
head(scorx)
tail(scorx)
plot(1:(p*(p-1)/2), scorx, type="l")
abline(h=0, col="red", lty=2) 

hist(scorx)

# plot correlations in X (just take 50 predictors at random)
randp <- sample(1:p, 50)
corrplot(corx[randp,randp], tl.pos='n', type="upper", diag=F)

# true predictions
truepred <- X %*% b
# variance of the error term according to the SNR
varerr <- var(truepred)/SNR
# generate the error
err <- rnorm(n)*sqrt(varerr)

# create the response
y <- truepred + err

# sanity check
varerr
var(err)
summary(lm(y ~ X[, 1:pnot]))
# summary(lm(y ~ X[, 1:p]))

y.train <- y
x.train <- X

```


We will now replicate the same analysis as before:
```{r}

# num of features retained by SIS/ISIS
# q <- n-1
q <- round(n / log(n))
q

# maximum num of iterations for ISIS
maxit <- 10

# vanilla SIS
model1 = SIS(x.train, y.train, 
             penalty = "lasso",
             varISIS = "vanilla", iter=F,
             tune = "cv", nfolds = 10, nsis=q,
             standardize = FALSE, seed = 1)

# vanilla ISIS
model2 = SIS(x.train, y.train,
             penalty = "lasso",
             varISIS = "vanilla", iter=T, iter.max=maxit,
             tune = "cv", nfolds = 10, nsis=q,
             standardize = FALSE, seed = 1)
```

Lets'see the selected predictors:
```{r}

cat("\n\n\n")
print(paste0("SIS selected features: ", length(model1$ix0)))
model1$ix0

cat("\n\n\n")
print(paste0("SIS+Lasso selected features: ", length(model1$ix)))
model1$ix

cat("\n\n\n")
print(paste0("ISIS selected features: ", length(model2$ix0)))
model2$ix0

cat("\n\n\n")
print(paste0("ISIS+Lasso selected features: ", length(model2$ix)))
model2$ix

```

Compare it with a Lasso fit.
Here is the overall path:
```{r}
modelLasso = glmnet(x.train, y.train, standardize = F)
plot(modelLasso, xvar="lambda")
```

Let's select the tuning parameter:
```{r}
set.seed(1)

modelLassoCV = cv.glmnet(x.train, y.train, standardize = F)
plot(modelLassoCV)

```

Extract the associated features
```{r}

modelLasso = glmnet(x.train, y.train, standardize = F, 
                    lambda=modelLassoCV$lambda.1se)
Lassocoef <- which(coef(modelLasso) != 0)
Lassocoef

# compare them with ISIS
Lassocoef %in% model1$ix

# compare them with ISIS
Lassocoef %in% model2$ix

```


Let's see if we can retrieve the "truly" relevant features using nonconvex penalization methods (MCP):
```{r}

# vanilla SIS
model1 = SIS(x.train, y.train, 
             penalty = "MCP",
             varISIS = "vanilla", iter=F,
             tune = "cv", nsis=q,
             standardize = FALSE, seed = 1)

# vanilla ISIS
model2 = SIS(x.train, y.train,
             penalty = "MCP",
             varISIS = "vanilla", iter=T, iter.max=maxit,
             tune = "cv", nsis=q,
             standardize = FALSE, seed = 1)
```

Let's see the results:
```{r}

cat("\n\n\n")
print(paste0("SIS selected features: ", length(model1$ix0)))
model1$ix0

cat("\n\n\n")
print(paste0("SIS+MCP selected features: ", length(model1$ix)))
model1$ix

cat("\n\n\n")
print(paste0("ISIS selected features: ", length(model2$ix0)))
model2$ix0

cat("\n\n\n")
print(paste0("ISIS+MCP selected features: ", length(model2$ix)))
model2$ix

```



\section{Logistic regression}

```{r}
set.seed(12345)
data("leukemia.train", package = "SIS")
data("leukemia.test", package = "SIS")
```

Let's construct our response variable and the design matrix.
```{r}
y1 <- leukemia.train[, dim(leukemia.train)[2]]
x1 <- as.matrix(leukemia.train[, -dim(leukemia.train)[2]])

x2 <- as.matrix(leukemia.test[, -dim(leukemia.test)[2]])
y2 <- leukemia.test[, dim(leukemia.test)[2]]
```

We further combine the training and test samples and then perform a 50%â€“50% random splitting of the observed data into new training and test data for which the number of cases remains balanced across these new samples (i.e. balanced sample splitting).

In this manner, the balanced training and test samples are of size 36.

```{r}

x <- rbind(x1, x2)
y <- c(y1, y2)
n <- dim(x)[1]
aux <- 1:n
ind.train1 <- sample(aux[y == 0], 23, replace = FALSE)
ind.train2 <- sample(aux[y == 1], 13, replace = FALSE)
ind.train <- c(ind.train1, ind.train2)

ind.test1 <- setdiff(aux[y == 0], ind.train1)
ind.test2 <- setdiff(aux[y == 1], ind.train2)
ind.test <- c(ind.test1, ind.test2)

```


Before variable screening and classification, we also standardize each predictor to zero mean and unit variance:
```{r}
y.train <- y[ind.train]
y.test <- y[ind.test]

x.train <- scale(x[ind.train, ])
x.test <- scale(x[ind.test, ])
```


We now perform variable selection using:
\begin{itemize}
  \item SIS: in its vanilla version
  \item ISIS: the iterated version of SIS
\end{itemize}
These are paired with the LASSO penalty and the ten-fold cross-validation method for choosing the regularization parameter.

```{r}

# vanilla SIS
model1 = SIS(x.train, y.train, 
             family = "binomial", 
             type.measure='auc',
             penalty = "lasso",
             varISIS = "vanilla", iter=F,
             tune = "cv", nfolds = 10, nsis=100,
             standardize = FALSE, seed = 9)

# vanilla ISIS
model2 = SIS(x.train, y.train, 
             family = "binomial", 
             type.measure='auc',
             penalty = "lasso",
             varISIS = "vanilla", iter=T, iter.max=2,
             tune = "cv", nfolds = 10, nsis=100,
             standardize = FALSE, seed = 9)
```

Lets'see the selected predictors:
```{r}

model1$ix
model2$ix

```


Compare it with a Lasso fit.
Here is the overall path:
```{r}
modelLasso = glmnet(x.train, y.train, family = "binomial",
                    standardize = F)
plot(modelLasso, xvar="lambda")
```

Let's select the tuning parameter:
```{r}
set.seed(1)

modelLassoCV = cv.glmnet(x.train, y.train, family = "binomial", 
                         standardize = F,
                         type.measure = "auc")
plot(modelLassoCV)

modelLassoCV$lambda.min


```

Extract the associated features
```{r}

modelLasso = glmnet(x.train, y.train, family = "binomial",
                    standardize = F,
                    type.measure = "class",
                    lambda=modelLassoCV$lambda.min)
Lassocoef <- which(coef(modelLasso) != 0)

# compare them with ISIS
Lassocoef %in% model2$ix

```


Let's now compare predictive power:
```{r}

# Make predictions on the testing data
print(paste0("Lasso selected features: ", length(Lassocoef)))
predLasso <- modelLasso %>% predict(x.test, type='class') %>% as.factor()
confusionMatrix(predLasso, as.factor(y.test))

cat("\n\n\n")
print(paste0("SIS selected features: ", length(model1$ix)))
predSIS <- model1 %>% SIS:::predict.SIS(x.test, type='class') %>% as.factor()
confusionMatrix(predSIS, as.factor(y.test))

cat("\n\n\n")
print(paste0("ISIS selected features: ", length(model2$ix)))
predISIS <- model2 %>% SIS:::predict.SIS(x.test, type='class') %>% as.factor()
confusionMatrix(predISIS, as.factor(y.test))

```


Let's have a look at correlation in the predictors:
```{r}
set.seed(1)
# just take 50 predictors at random
randp <- sample(1:ncol(x.train), 50)
corrplot(cor(x.train)[randp,randp], tl.pos='n', type="upper", diag=F)
```


Note that it is easy to build more involved versions of SIS, such as:
\begin{itemize}
  \item Var1-ISIS: a first variant of ISIS
  \item Perm-var-ISIS: permutation-based variant with sample splitting
  \item etc. 
\end{itemize}
```{r}

 
# # Var1-ISIS
# model3 = SIS(x.train, y.train, family = "binomial", penalty = "lasso",
#              tune = "cv", nfolds = 10, nsis = 100, varISIS = "aggr", seed = 9,
#              standardize = FALSE)
# 
# # Perm-var-ISIS
# model4 = SIS(x.train, y.train, family = "binomial", penalty = "lasso",
#              tune = "cv", nfolds = 10, nsis = 100, varISIS = "aggr", perm = TRUE,
#              q = 0.95, seed = 9, standardize = FALSE)


# selected predictors:
# model3$ix
# model4$ix

# predictive power:
# predISISv1 <- model3 %>% SIS:::predict.SIS(x.test, type='class') %>% as.factor()
# confusionMatrix(predISISv1, as.factor(y.test))
# 
# predISISperm <- model4 %>% SIS:::predict.SIS(x.test, type='class') %>% as.factor()
# confusionMatrix(predISISperm, as.factor(y.test))
```