---
title: "*Cross Validation*"
author: "S.Tonini, F. Chiaromonte (special thanks to J. Di Iorio and L. Insolia)"
date: "February 28th 2023"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
```


\section{Introduction}

\subsection{Libraries}

We are going to use:
\begin{itemize}
  \item \textbf{tidyverse}: \textit{Easily Install and Load the 'Tidyverse'}
  \item \textbf{caret}: \textit{Classification and Regression Training}
  \item \textbf{ggplot2}: \textit{Create Elegant Data Visualisations Using the Grammar of Graphics}
  \item \textbf{dslabs}: \textit{Data Science lab}
\end{itemize}

```{r}
library(tidyverse)  # data manipulation and visualization
library(caret)      # statistical learning techniques
library(ggplot2)    # plots
library(dslabs)     # digit recognition data
```


\subsection{Data}

We will use a dataset already presented in a previous lesson (the \textbf{economics} in \textbf{ggplot2} package) as well as the \verb|mnist_27| dataset which is part of the \textbf{dslabs} package.

The \textbf{mnist} dataset is a very famous and large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. In the \verb|mnist_27| data set, we only include a randomly selected set of 2's and 7's (the label/class that we try to predict) along with the two predictors based on the proportion of dark pixels in the upper left and lower right quadrants respectively. The dataset is divided into training and test sets.

```{r}
data("mnist_27")
mnist_27$test %>% ggplot(aes(x_1, x_2, color = y)) +  geom_point()
```


\section{Cross validation}

We are going to perform CV by hand. Precisely we are going to perform:
\begin{enumerate}
  \item Leave-one-out cross validation (LOOCV)
  \item $k$-folds cross validation
\end{enumerate}

Before starting let's prepare the data.
```{r}
data(economics)
economics <- economics[dim(economics)[1]:(dim(economics)[1]-119),]
economics$index <- 1:120
plot(economics$uempmed)
```

\subsection{LOOCV}
In this approach, we reserve only one data point from the available dataset, and train the model on the rest of the data. This process iterates for each data point. 

```{r}

set.seed(2022)

degree_list <- list()
span_values <- seq(0.05,1,length=10)
for(deg in 1:2){ #polynomial degree
  err <- list()
  for(k in 1:length(span_values)){ #smoothness
    score <- list()
    for(i in 1:(nrow(economics))){
      training = economics[-i,]
      model = loess(uempmed ~ index, data = training, span = span_values[k], degree=deg) 
      validation = economics[i,]
      pred = predict(model, validation)
      # error of ith fold
      score[[i]] = (validation$uempmed - pred)^2
    }
  # returns a vector with the average error for a given degree & span
  err[[k]] <- mean(unlist(score),na.rm=TRUE) 
  }
  degree_list[[deg]] <- err
}

# prepare dataframe for ggplot
spans <- rep(span_values,2)
degrees <- rep(c(1,2), each = length(span_values))
err <- unlist(degree_list)
df_toplot <- as.data.frame(cbind(spans,degrees,err))

# plot
p <- ggplot(df_toplot, aes(x=spans, y=err, group=factor(degrees))) + 
  geom_point() + geom_line(aes(col=factor(degrees)))
p
```

Let's find the parameters corresponding to the minimum error.
```{r}
best <- df_toplot[which(df_toplot$err==min(df_toplot$err)),]
best
```
Let's plot the resulting smoothed curve.
```{r}
res <- loess(uempmed ~ index, data = economics, span =best$spans, degree=best$degrees) 
plot(economics$index, economics$uempmed)
lines(predict(res), col='blue')
```

\subsection{$k$-fold CV}

Let's validate the parameter using the $k$-fold cross validation.

These are the steps we need to implement:
\begin{enumerate}
  \item Randomly split your entire dataset into $k$ folds;
  \item Iterate across each $k$th fold, which serves as a testing set, and train your model only on the remaining $k–1$ folds;
  \item Test model accuracy/effectiveness on the $k$th fold, and record the ``error'' you see on each of the $k$ predictions;
  \item Repeat this until each of the $k$-folds has served as the test set;
  \item The average of your $k$ recorded errors is called the \textbf{cross-validation error} and will serve as a performance metric for the model.
\end{enumerate}

Create the folds:
```{r}
flds <- caret::createFolds(1:nrow(economics), 
                           k = 3, list = TRUE, 
                           returnTrain = FALSE)
flds
class(flds)
# yopu can use [[k]] or [k] to access the k-th element
flds[1]     # you need to "unlist" afterwards (see below)
flds[[1]]   # you do not

```

Perform an iteration similarly to the one for LOOCV:
```{r}
degree_list <- list()
for(deg in 1:2){ #polynomial degree
  err <- list()
  for(k in 1:length(span_values)){ #smoothness
    score <- list()
    for(i in 1:length(flds)){
      validation <- economics[unlist(flds[i]),]
      training <- economics[unlist(flds[-i]),]
      model = loess(uempmed ~ index, data = training, span = span_values[k], degree=deg) 
      pred = predict(model, validation)
      score[[i]] <- mean((pred - validation$uempmed)^2, na.rm=TRUE)
    }
    err[[k]] <- mean(unlist(score))
  }
  degree_list[[deg]] <- unlist(err)
}


spans <- rep(span_values,2)
degrees <- rep(c(1,2), each = length(span_values))
err <- unlist(degree_list)
df_toplot <- as.data.frame(cbind(spans,degrees,err))


p <- ggplot(df_toplot, aes(x=spans, y=err, group=factor(degrees))) + 
  geom_point() + geom_line(aes(col=factor(degrees)))
p
```

Let us find the parameters corresponding to the minimum error.
```{r}
df_toplot[which(df_toplot$err==min(df_toplot$err)),]
```

Let us plot the resulting regression line.
```{r}
best <- df_toplot[which(df_toplot$err==min(df_toplot$err)),]
res <- loess(uempmed ~ index, data = economics, span =best$spans, degree=best$degrees) 
plot(economics$index, economics$uempmed)
lines(predict(res), col='blue')
```

\section{Using caret for CV}

Cross validation is also implemented in the train function of the caret package. 
Here caret train function allows one to train different algorithms using the same syntax. 

So, for example, we can type:
```{r}
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
confusionMatrix(y_hat_knn, mnist_27$test$y)
```


In the presence of a a tuning parameter, it automatically uses cross validation to decide among a few default values. You can quickly see the results of the cross validation using the \textbf{ggplot} function. The argument \textbf{highlight} highlights the max:

```{r}
ggplot(train_knn, highlight = TRUE)
```

By default, caret used bootstrap resampling with 25 repetitions – this is the default resampling approach in caret. The process caret used was:

\begin{itemize}
\item Randomly sample the data with replacement. This means that a single observation could be chosen more than once. The total size of the modeling dataset will be the same as the original dataset but some observations will not be included and some will be included more than once.
\item Develop a model based on the randomly sampled observations only.
\item Use the withheld observations (also known as “out-of-bag” (OOB) observations) to compute the Accuracy based on the predictions.
\item Repeat the process (25 models are run) and average the Accuracy values.
\item When the resampling is done, caret then runs a final model on the full dataset and stores this in finalModel. So, in this example, 25 + 1 models are run, 25 bootstraps and one model run using full dataset.
\end{itemize}


Note: the Accuracy is the percentage of correctly classified OOB observations.

For the kNN method, the default is to try $k= (5,7,9)$. 

We can change this using the \textbf{tuneGrid} parameter.
```{r}
set.seed(2022)
train_knn <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)
```

The best $k$ shown in the plot and the corresponding training set outcome distribution is accessible as follows:
```{r}
train_knn$bestTune
train_knn$finalModel
```

The overall accuracy on the training set is:
```{r}
confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"),mnist_27$test$y)
```

we can use k-folds cross-validation instead of bagging by setting

```{r}
set.seed(2023)
tc <- trainControl(method = "cv", number = 10)

train_kCV <- train(y ~ ., method = "knn", 
                   data = mnist_27$train,
                   trControl = tc)
ggplot(train_kCV, highlight = TRUE)

```

\section{Temporal Block Cross Validation}


Simple random sampling is probably not the best way to resample time series data. \textbf{caret} contains a function called \textbf{createTimeSlices} that can create the indices for this type of splitting.

The function takes as input a vector and three parameters:
\begin{itemize}
    \item $y$: a vector of outcomes. These should be in chronological order
    \item $initialWindow$: the initial number of consecutive values in each training set sample
    \item $horizon$: The number of consecutive values in test set sample
    \item $fixedWindow$: A logical: if FALSE, the training set always start at the first sample and the training set size will vary over data splits.
\end{itemize}


Example 1: one training set and one test set
```{r}

p <- 0.75

createTimeSlices(y = 1:nrow(economics), 
                 initialWindow = round(p*nrow(economics),0), 
                 horizon = (nrow(economics)-round(p*nrow(economics),0)), 
                 fixedWindow = TRUE)           # in this example fixedWindow doesn't matter!!

```

Example 2: rolling window
```{r}

createTimeSlices(y = 1:nrow(economics), 
                 initialWindow = round(p*nrow(economics),0), 
                 horizon = ((nrow(economics)-round(p*nrow(economics),0))-2), 
                 fixedWindow = TRUE)

```

Example 3: recursive window
```{r}

createTimeSlices(y = 1:nrow(economics), 
                 initialWindow = round(p*nrow(economics),0), 
                 horizon = ((nrow(economics)-round(p*nrow(economics),0))-2), 
                 fixedWindow = FALSE)

```


Tip for Mod 2!!!

The function \textbf{trainControl} (in \textbf{caret}) controls the computational setup of the \textbf{train} function.

The created object can be placed in the \textbf{trControl} argument.

```{r}
#trainControl(method = "timeslice",
#             initialWindow = 90,
#             horizon = 25,
#             fixedWindow = FALSE)
```
